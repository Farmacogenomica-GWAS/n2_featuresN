{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of the Results File: n2_featuresN Obtained via Tierpsy Tracker"
      ],
      "metadata": {
        "id": "9xpb3lTC2pZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive connection"
      ],
      "metadata": {
        "id": "IJY_9g242ute"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytic0M3p2uHq",
        "outputId": "6f052b56-153f-4920-cd60-aaeda2950d98"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "wK-ELC0i2x9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "QkFrBF9N22ZI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting file content"
      ],
      "metadata": {
        "id": "KYdGuAmk249w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_hdf5_datasets(file_path):\n",
        "    \"\"\"\n",
        "    Inspects an HDF5 file and prints the names and sizes of its datasets.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Inspecting file: {file_path}\")\n",
        "    print(f\"{'='*30}\")\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return  # Exit the function if the file doesn't exist\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdfid:\n",
        "            datasets_info = {}\n",
        "            for name, obj in hdfid.items():\n",
        "                if isinstance(obj, h5py.Dataset):\n",
        "                    datasets_info[name] = obj.size\n",
        "\n",
        "            if datasets_info:\n",
        "                print(\"Datasets found and their sizes:\")\n",
        "                for name, size in datasets_info.items():\n",
        "                    print(f\"  Dataset: {name}, Size: {size} elements\")\n",
        "            else:\n",
        "                print(\"No top-level datasets were found in this file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the file '{file_path}': {e}\")\n",
        "\n",
        "# Define the file path you want to analyze here.  REPLACE THIS!\n",
        "file_to_analyze = '/content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/n2_featuresN.hdf5'  # <--- REPLACE THIS LINE\n",
        "\n",
        "inspect_hdf5_datasets(file_to_analyze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sofkBrh-26e0",
        "outputId": "b142e270-252b-488a-c0c6-d6889c16d711"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Inspecting file: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/n2_featuresN.hdf5\n",
            "==============================\n",
            "Datasets found and their sizes:\n",
            "  Dataset: blob_features, Size: 23324 elements\n",
            "  Dataset: features_stats, Size: 4539 elements\n",
            "  Dataset: timeseries_data, Size: 23324 elements\n",
            "  Dataset: trajectories_data, Size: 23324 elements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting the datasets to individuals CSV files"
      ],
      "metadata": {
        "id": "30qR2lsw3Efj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_hdf5_datasets_to_csv(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Exports all datasets from an HDF5 file to individual CSV files.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The path to the directory where the CSV files will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Exporting all datasets from: {file_path}\")\n",
        "    print(f\"CSV files will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            for dataset_name in hdf_file:\n",
        "                if isinstance(hdf_file[dataset_name], h5py.Dataset):\n",
        "                    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
        "                    dataset = hdf_file[dataset_name]\n",
        "                    data = dataset[:]  # Read all data\n",
        "\n",
        "                    # Convert to Pandas DataFrame\n",
        "                    df = pd.DataFrame(data)\n",
        "\n",
        "                    # Construct the CSV file path\n",
        "                    csv_file_path = os.path.join(output_dir, f\"{dataset_name}.csv\")\n",
        "\n",
        "                    # Export to CSV\n",
        "                    df.to_csv(csv_file_path, index=False)\n",
        "                    print(f\"Dataset '{dataset_name}' successfully exported to: {csv_file_path}\")\n",
        "                else:\n",
        "                    print(f\"Skipping: '{dataset_name}' is not a dataset.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nData export process complete.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:  MODIFY THESE PATHS APPROPRIATELY\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/n2_featuresN.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Datasets'  # <--- Replace with the desired output folder\n",
        "\n",
        "    export_hdf5_datasets_to_csv(hdf5_file_path, csv_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUK5cpRZ3FC5",
        "outputId": "f8007871-8069-40cc-8623-61944237e9ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Exporting all datasets from: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/n2_featuresN.hdf5\n",
            "CSV files will be saved in: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Datasets\n",
            "==============================\n",
            "\n",
            "Processing dataset: blob_features\n",
            "Dataset 'blob_features' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Datasets/blob_features.csv\n",
            "Skipping: 'coordinates' is not a dataset.\n",
            "\n",
            "Processing dataset: features_stats\n",
            "Dataset 'features_stats' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Datasets/features_stats.csv\n",
            "Skipping: 'provenance_tracking' is not a dataset.\n",
            "\n",
            "Processing dataset: timeseries_data\n",
            "Dataset 'timeseries_data' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Datasets/timeseries_data.csv\n",
            "\n",
            "Processing dataset: trajectories_data\n",
            "Dataset 'trajectories_data' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Datasets/trajectories_data.csv\n",
            "\n",
            "Data export process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A problem arose because 'coordinates' and 'provenance_tracking' were not datasets. They were extracted separately:"
      ],
      "metadata": {
        "id": "FX3Qcque3WKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note on non-dataset exports:\n",
        "#\n",
        "# The script exports standard HDF5 datasets to CSV files.  However, the following non-dataset items were handled specially:\n",
        "#\n",
        "# -  'coordinates': This HDF5 group contains 3D datasets ('dorsal_contours', 'skeletons', 'ventral_contours').\n",
        "#    These datasets were extracted and stored as JSON strings within individual CSV files to preserve their 3D structure.\n",
        "#\n",
        "# -  'provenance_tracking': This HDF5 group contains metadata attributes ('CLASS', 'TITLE', 'VERSION').\n",
        "#    These attributes were extracted and written to a single-row CSV file."
      ],
      "metadata": {
        "id": "lS8pvSbw3Xl5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Coordinates extraction"
      ],
      "metadata": {
        "id": "1nzbJF3C3W76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_coordinates(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Extracts the 'dorsal_contours', 'skeletons', and 'ventral_contours' datasets from an HDF5 file,\n",
        "    handling their 3D structure by storing them as JSON strings in a CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The path to the directory where the CSV files will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Extracting coordinates from: {file_path}\")\n",
        "    print(f\"CSV files will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            if 'coordinates' in hdf_file:\n",
        "                coordinates_group = hdf_file['coordinates']\n",
        "                for dataset_name in ['dorsal_contours', 'skeletons', 'ventral_contours']:\n",
        "                    if dataset_name in coordinates_group:\n",
        "                        print(f\"\\nProcessing dataset: {dataset_name}\")\n",
        "                        data = coordinates_group[dataset_name][:]  # Read all data\n",
        "\n",
        "                        if data.ndim > 2:\n",
        "                            # Store 3D coordinates as JSON strings in CSV\n",
        "                            print(f\"  Storing 3D coordinates '{dataset_name}' as JSON strings in CSV.\")\n",
        "                            # Convert each 2D slice to a JSON string\n",
        "                            json_data = [json.dumps(slice_2d.tolist()) for slice_2d in data]\n",
        "                            df = pd.DataFrame({dataset_name: json_data})  # Store in a DataFrame\n",
        "                            csv_file_path = os.path.join(output_dir, f\"{dataset_name}.csv\")\n",
        "                            df.to_csv(csv_file_path, index=False)\n",
        "                            print(f\"  Dataset '{dataset_name}' successfully exported to: {csv_file_path}\")\n",
        "                        else:\n",
        "                            print(f\"  Warning: Dataset '{dataset_name}' is not 3D. Skipping.\")\n",
        "                    else:\n",
        "                        print(f\"  Warning: Dataset '{dataset_name}' not found in 'coordinates' group.\")\n",
        "            else:\n",
        "                print(f\"  Warning: 'coordinates' group not found in HDF5 file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nCoordinate extraction process complete.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/n2_featuresN.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Groups'  # <--- Replace with the desired output folder\n",
        "\n",
        "    extract_coordinates(hdf5_file_path, csv_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsYMuiWO3dTS",
        "outputId": "e2894038-138b-4bb2-ac95-331fe425f48f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Extracting coordinates from: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/n2_featuresN.hdf5\n",
            "CSV files will be saved in: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Groups\n",
            "==============================\n",
            "\n",
            "Processing dataset: dorsal_contours\n",
            "  Storing 3D coordinates 'dorsal_contours' as JSON strings in CSV.\n",
            "  Dataset 'dorsal_contours' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Groups/dorsal_contours.csv\n",
            "\n",
            "Processing dataset: skeletons\n",
            "  Storing 3D coordinates 'skeletons' as JSON strings in CSV.\n",
            "  Dataset 'skeletons' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Groups/skeletons.csv\n",
            "\n",
            "Processing dataset: ventral_contours\n",
            "  Storing 3D coordinates 'ventral_contours' as JSON strings in CSV.\n",
            "  Dataset 'ventral_contours' successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Groups/ventral_contours.csv\n",
            "\n",
            "Coordinate extraction process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Provenance_tracking extraction"
      ],
      "metadata": {
        "id": "gh00NCVt3zgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_provenance_tracking(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Extracts the attributes from the 'provenance_tracking' group in an HDF5 file\n",
        "    and saves them to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The directory where the CSV file will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Extracting provenance_tracking from: {file_path}\")\n",
        "    print(f\"CSV file will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            if 'provenance_tracking' in hdf_file:\n",
        "                provenance_group = hdf_file['provenance_tracking']\n",
        "                attributes = {}\n",
        "                for attr_name, attr_value in provenance_group.attrs.items():\n",
        "                    attributes[attr_name] = attr_value\n",
        "                df = pd.DataFrame([attributes])  # Create a DataFrame with a single row\n",
        "                csv_file_path = os.path.join(output_dir, \"provenance_tracking.csv\")\n",
        "                df.to_csv(csv_file_path, index=False)\n",
        "                print(f\"Provenance tracking data successfully exported to: {csv_file_path}\")\n",
        "            else:\n",
        "                print(f\"Warning: 'provenance_tracking' group not found in HDF5 file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nProvenance tracking extraction process complete.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/n2_featuresN.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Groups'  # <--- Replace with the desired output folder\n",
        "\n",
        "    extract_provenance_tracking(hdf5_file_path, csv_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pia_Bys333VB",
        "outputId": "cbee03fb-4537-47a3-eb75-eb2687e8a0b6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Extracting provenance_tracking from: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/n2_featuresN.hdf5\n",
            "CSV file will be saved in: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Groups\n",
            "==============================\n",
            "Provenance tracking data successfully exported to: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Groups/provenance_tracking.csv\n",
            "\n",
            "Provenance tracking extraction process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of each dataset"
      ],
      "metadata": {
        "id": "_4q9psFKLigD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Top-Level Datasets"
      ],
      "metadata": {
        "id": "mba453mNLm11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Datasets'\n",
        "\n",
        "print(f\"\\n{'='*30}\")\n",
        "print(f\"Visualizing the first 5 rows of the datasets in: {csv_output_directory}\")\n",
        "print(f\"{'='*30}\")\n",
        "try:\n",
        "    # Check if the output directory exists\n",
        "    if not os.path.exists(csv_output_directory):\n",
        "        print(f\"Error: The directory '{csv_output_directory}' was not found.\")\n",
        "        exit()\n",
        "\n",
        "    # Iterate through all files in the specified directory\n",
        "    for filename in os.listdir(csv_output_directory):\n",
        "        if filename.endswith(\".csv\"):  # Check if the file is a CSV file\n",
        "            file_path = os.path.join(csv_output_directory, filename)\n",
        "            try:\n",
        "                # Read the CSV file into a Pandas DataFrame\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                print(f\"\\n--- File: {filename} ---\")\n",
        "                print(\"First 5 rows:\")\n",
        "                if not df.empty:\n",
        "                    print(df.head())\n",
        "                else:\n",
        "                    print(\"The DataFrame is empty.\")\n",
        "\n",
        "            except pd.errors.EmptyDataError:\n",
        "                print(f\"Warning: The file '{filename}' is empty.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading the file '{filename}': {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"\\nDataset visualization process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzHvkI2_LpcI",
        "outputId": "8b2481b2-8ece-4629-dca6-ee557976f89a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Visualizing the first 5 rows of the datasets in: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Datasets\n",
            "==============================\n",
            "\n",
            "--- File: blob_features.csv ---\n",
            "First 5 rows:\n",
            "       coord_x      coord_y   area   perimeter  box_length  box_width  \\\n",
            "0  1647.895996  1662.704834  368.5  140.124893   54.063095  17.351398   \n",
            "1  1648.061401  1662.817749  354.5  138.124893   53.835423  16.643318   \n",
            "2  1647.644165  1662.226440  347.0  136.225403   53.534157  16.949924   \n",
            "3  1647.155151  1660.760986  359.0  137.539108   51.894089  16.069649   \n",
            "4  1647.464355  1660.255371  371.5  136.468033   54.126434  15.504248   \n",
            "\n",
            "   quirkiness  compactness  box_orientation  solidity  intensity_mean  \\\n",
            "0    0.947097     0.235840       -32.125000  0.491333      114.920746   \n",
            "1    0.951013     0.233497       -32.735230  0.494766      114.497580   \n",
            "2    0.948553     0.234976       -32.471190  0.487018      113.791150   \n",
            "3    0.950847     0.238480       -25.016895  0.504923      114.124405   \n",
            "4    0.958097     0.250673       -24.537730  0.520308      113.737816   \n",
            "\n",
            "   intensity_std       hu0       hu1       hu2       hu3       hu4       hu5  \\\n",
            "0       8.030436  0.628597  0.311084  0.054412  0.006758 -0.000123 -0.003510   \n",
            "1       8.289959  0.692604  0.390438  0.054949  0.007130 -0.000118 -0.003758   \n",
            "2       8.065038  0.694281  0.395342  0.049629  0.006184 -0.000107 -0.003886   \n",
            "3       8.527963  0.641227  0.332233  0.039731  0.005265 -0.000070 -0.002836   \n",
            "4       8.547855  0.687632  0.386623  0.045736  0.006054 -0.000091 -0.003593   \n",
            "\n",
            "        hu6  \n",
            "0 -0.000042  \n",
            "1 -0.000077  \n",
            "2  0.000016  \n",
            "3 -0.000029  \n",
            "4  0.000042  \n",
            "\n",
            "--- File: features_stats.csv ---\n",
            "First 5 rows:\n",
            "                                                name      value\n",
            "0                                      b'speed_10th' -10.522734\n",
            "1                       b'angular_velocity_abs_10th'   0.018587\n",
            "2         b'relative_to_body_speed_midbody_abs_10th'   0.171267\n",
            "3  b'relative_to_body_radial_velocity_head_tip_10th'  -7.287537\n",
            "4  b'relative_to_body_angular_velocity_head_tip_a...   0.043208\n",
            "\n",
            "--- File: timeseries_data.csv ---\n",
            "First 5 rows:\n",
            "   worm_index  timestamp well_name      speed  angular_velocity  \\\n",
            "0           2          0    b'n/a'        NaN               NaN   \n",
            "1           2          1    b'n/a'   9.264768         -0.235506   \n",
            "2           2          2    b'n/a'  14.667897         -0.395998   \n",
            "3           2          3    b'n/a'  13.350494         -0.493967   \n",
            "4           2          4    b'n/a'   4.674884         -0.210109   \n",
            "\n",
            "   relative_to_body_speed_midbody  relative_to_body_radial_velocity_head_tip  \\\n",
            "0                             NaN                                        NaN   \n",
            "1                       -1.666544                                  -7.034277   \n",
            "2                       -1.353861                                  -5.006540   \n",
            "3                       -0.582070                                  -3.709202   \n",
            "4                        0.424021                                   3.060920   \n",
            "\n",
            "   relative_to_body_angular_velocity_head_tip  \\\n",
            "0                                         NaN   \n",
            "1                                    0.223148   \n",
            "2                                    0.113109   \n",
            "3                                    0.074669   \n",
            "4                                   -0.166262   \n",
            "\n",
            "   relative_to_body_radial_velocity_neck  \\\n",
            "0                                    NaN   \n",
            "1                              -1.220187   \n",
            "2                              -0.109703   \n",
            "3                               0.156479   \n",
            "4                               0.207844   \n",
            "\n",
            "   relative_to_body_angular_velocity_neck  ...  turn  head_tail_distance  \\\n",
            "0                                     NaN  ...   0.0           52.116245   \n",
            "1                               -0.145304  ...   0.0           52.734250   \n",
            "2                               -0.187879  ...   0.0           52.671646   \n",
            "3                               -0.183326  ...   0.0           50.074470   \n",
            "4                               -0.067105  ...   0.0           53.272690   \n",
            "\n",
            "   coord_x_body  coord_y_body  coord_x_tail  coord_y_tail  coord_x_midbody  \\\n",
            "0     1646.2357     1664.8234     1664.7633     1680.2966        1645.4786   \n",
            "1     1646.5070     1665.2754     1664.8093     1680.3412        1645.8617   \n",
            "2     1646.2993     1664.7266     1664.4161     1680.0034        1645.7997   \n",
            "3     1644.8118     1662.4364     1660.8818     1679.6084        1644.5253   \n",
            "4     1644.4617     1661.3793     1660.5130     1680.0765        1644.0880   \n",
            "\n",
            "   coord_y_midbody  coord_x_head  coord_y_head  \n",
            "0        1665.2960     1638.9560     1639.9805  \n",
            "1        1665.7699     1639.2073     1640.4607  \n",
            "2        1665.1212     1638.8221     1640.4104  \n",
            "3        1662.7428     1640.9647     1638.8129  \n",
            "4        1661.5245     1641.6714     1637.2584  \n",
            "\n",
            "[5 rows x 153 columns]\n",
            "\n",
            "--- File: trajectories_data.csv ---\n",
            "First 5 rows:\n",
            "   timestamp_raw  timestamp_time  worm_index_joined    coord_x    coord_y  \\\n",
            "0              0             0.0                  2  1648.0538  1663.1330   \n",
            "1              1             0.1                  2  1647.4797  1662.4630   \n",
            "2              2             0.2                  2  1647.0740  1661.9379   \n",
            "3              3             0.3                  2  1646.7949  1661.5143   \n",
            "4              4             0.4                  2  1646.6013  1661.1490   \n",
            "\n",
            "   threshold  roi_size   area  frame_number  was_skeletonized  skeleton_id  \\\n",
            "0      128.1      68.0  366.5             0                 1            0   \n",
            "1      128.1      68.0  366.5             1                 1            1   \n",
            "2      128.1      68.0  366.5             2                 1            2   \n",
            "3      128.1      68.0  367.0             3                 1            3   \n",
            "4      128.1      68.0  367.0             4                 1            4   \n",
            "\n",
            "   old_trajectory_data_index  worm_label  worm_index_manual  has_skeleton  \n",
            "0                          0         0.0                  2             1  \n",
            "1                          1         0.0                  2             1  \n",
            "2                          2         0.0                  2             1  \n",
            "3                          3         0.0                  2             1  \n",
            "4                          4         0.0                  2             1  \n",
            "\n",
            "Dataset visualization process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Datasets extracted from the groups"
      ],
      "metadata": {
        "id": "FvfC6ShYLvso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Groups'\n",
        "\n",
        "print(f\"\\n{'='*30}\")\n",
        "print(f\"Visualizing the first 5 rows of the datasets in: {csv_output_directory}\")\n",
        "print(f\"{'='*30}\")\n",
        "try:\n",
        "    # Check if the output directory exists\n",
        "    if not os.path.exists(csv_output_directory):\n",
        "        print(f\"Error: The directory '{csv_output_directory}' was not found.\")\n",
        "        exit()\n",
        "\n",
        "    # Iterate through all files in the specified directory\n",
        "    for filename in os.listdir(csv_output_directory):\n",
        "        if filename.endswith(\".csv\"):  # Check if the file is a CSV file\n",
        "            file_path = os.path.join(csv_output_directory, filename)\n",
        "            try:\n",
        "                # Read the CSV file into a Pandas DataFrame\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                print(f\"\\n--- File: {filename} ---\")\n",
        "                print(\"First 5 rows:\")\n",
        "                if not df.empty:\n",
        "                    print(df.head())\n",
        "                else:\n",
        "                    print(\"The DataFrame is empty.\")\n",
        "\n",
        "            except pd.errors.EmptyDataError:\n",
        "                print(f\"Warning: The file '{filename}' is empty.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading the file '{filename}': {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"\\nDataset visualization process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_vYyWuKLysl",
        "outputId": "2e6fa30a-9b34-4b77-9b30-aa43bdf241b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Visualizing the first 5 rows of the datasets in: /content/drive/MyDrive/Worms/Resultados/n2/n2_featuresN/Groups\n",
            "==============================\n",
            "\n",
            "--- File: provenance_tracking.csv ---\n",
            "First 5 rows:\n",
            "      CLASS                     TITLE VERSION\n",
            "0  b'GROUP'  Empty(dtype=dtype('S1'))  b'1.0'\n",
            "\n",
            "--- File: dorsal_contours.csv ---\n",
            "First 5 rows:\n",
            "                                     dorsal_contours\n",
            "0  [[1641.0015869140625, 1636.00830078125], [1639...\n",
            "1  [[1640.0, 1636.0], [1638.9564208984375, 1637.0...\n",
            "2  [[1640.0167236328125, 1635.99853515625], [1638...\n",
            "3  [[1642.9984130859375, 1634.99609375], [1641.51...\n",
            "4  [[1642.0, 1633.0], [1640.9749755859375, 1634.0...\n",
            "\n",
            "--- File: skeletons.csv ---\n",
            "First 5 rows:\n",
            "                                           skeletons\n",
            "0  [[1641.0260009765625, 1636.0228271484375], [16...\n",
            "1  [[1639.95947265625, 1635.9832763671875], [1640...\n",
            "2  [[1640.028564453125, 1636.0118408203125], [163...\n",
            "3  [[1642.9951171875, 1634.9979248046875], [1643....\n",
            "4  [[1641.9586181640625, 1632.9881591796875], [16...\n",
            "\n",
            "--- File: ventral_contours.csv ---\n",
            "First 5 rows:\n",
            "                                    ventral_contours\n",
            "0  [[1641.009765625, 1635.9959716796875], [1641.8...\n",
            "1  [[1639.9925537109375, 1636.0125732421875], [16...\n",
            "2  [[1640.0142822265625, 1635.9857177734375], [16...\n",
            "3  [[1643.0142822265625, 1634.9857177734375], [16...\n",
            "4  [[1641.9844970703125, 1632.998046875], [1643.2...\n",
            "\n",
            "Dataset visualization process completed.\n"
          ]
        }
      ]
    }
  ]
}